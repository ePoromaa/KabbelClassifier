{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Käbbel importerare\n",
    "\n",
    "Denna funktion läser allt käbbel från regeringens hemsida och sparar ner skiten i en fil: kabbel.txt\n",
    "- datasize kan sättas till hur många sidor som ska hämtas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Läs in fil och skapa ordvektor\n",
    "\n",
    "Denna läser in orden från ovan skapade fil och bygger en data av den som vi använder som träningsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib.request, json, re\n",
    "import pprint as pp\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def file_to_json(filename):\n",
    "    with open(filename,encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "        return data\n",
    "\n",
    "def kabbel_to_vec(jsondata):\n",
    "    speech = []\n",
    "    classes = []\n",
    "    person = []\n",
    "    words = []\n",
    "    for sample in jsondata:\n",
    "        cleaned = re.sub(r'[.|,]', '', sample[\"kabbel\"])\n",
    "        words += cleaned.lower().split()\n",
    "        speech.append(cleaned)\n",
    "        classes.append(sample[\"parti\"])\n",
    "        person.append(sample[\"talare\"])\n",
    "\n",
    "    #print(speech)\n",
    "    #data = tf.compat.as_str(speech).split()\n",
    "    return speech, classes, person\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Läs in olika delar och tilldela X och Y i vår modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exempel på ett sample ur datan: \n",
      "[\n",
      "    {\n",
      "        \"kabbel\": \"jag ska fatta mig kort.\",\n",
      "        \"parti\": \"SD\",\n",
      "        \"talare\": \"Paula Bieler (SD)\"\n",
      "    },\n",
      "    {\n",
      "        \"kabbel\": \"jag har redan redogjort f\\u00f6r huvudargumentationen bakom detta,\",\n",
      "        \"parti\": \"SD\",\n",
      "        \"talare\": \"Paula Bieler (SD)\"\n",
      "    }\n",
      "]\n",
      "Classes: ['SD', 'SD', 'SD']... \n",
      "\n",
      "Speech:['jag ska fatta mig kort', 'jag har redan redogjort för huvudargumentationen bakom detta', 'men för att sammanfatta kan vi konstatera att vi har en situation i dagsläget där endera av två olika scenarier har utspelat sig de senaste åren']...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "jsondata = file_to_json(\"kabbel_big.txt\")\n",
    "print(\"Exempel på ett sample ur datan: \\n\"+json.dumps(jsondata[:2], indent=4))\n",
    "\n",
    "\n",
    "speech, classes, person = kabbel_to_vec(jsondata)\n",
    "\n",
    "print(\"Classes: {0}... \\n\\nSpeech:{1}...\".format(classes[:3], speech[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anpassning av träningsdata (sample=X) och klassificering (Y)\n",
    "\n",
    "Vi konverterar varje sample till en träningsvektor av längden `len(unika antal ord)` och värde `n` för antal förekomst per ord\n",
    "\n",
    "Vi konverterar varje klasificering till en vektor som beskriver klasserna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def samples_to_vocab(samples):\n",
    "    all_unique_words = list(set(' '.join(samples).split(' '))) \n",
    "    word_to_idx = {i:w for w,i in enumerate(all_unique_words)}\n",
    "    idx_to_word = {i:w for i,w in enumerate(all_unique_words)}\n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "def sample_to_vec(sample, wi):\n",
    "    vocabular = wi\n",
    "    sample_words = np.array(sample.split(\" \"))\n",
    "    vec = np.array([wi[x] for x in sample_words])\n",
    "    one_hot = np.zeros([len(vec), len(vocabular)])\n",
    "    one_hot[np.arange(len(vec)),vec] = 1\n",
    "    one_hot = np.sum(one_hot,axis=0)\n",
    "    return vec,one_hot\n",
    "\n",
    "def samples_to_train(samples):\n",
    "    vocabulary, iw = samples_to_vocab(samples)\n",
    "    sample_vector = []\n",
    "    for sample in samples:\n",
    "        _, x = sample_to_vec(sample, vocabulary)\n",
    "        sample_vector.append(x)\n",
    "    return np.matrix(sample_vector).reshape((len(samples),len(vocabulary))).T, vocabulary, iw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jag ska fatta mig kort', 'jag har redan redogjort för huvudargumentationen bakom detta']\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(matrix([[ 0.,  1.],\n",
       "         [ 1.,  0.]]), {'Björn': 0, 'Joakim': 1}, {0: 'Björn', 1: 'Joakim'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data =[\n",
    "    {\n",
    "        \"talare\": \"Joakim\",\n",
    "        \"parti\": \"S\",\n",
    "        \"kabbel\": \"jag ska, fatta mig kort.\"\n",
    "    },\n",
    "    {\n",
    "        \"talare\": \"Björn\",\n",
    "        \"parti\": \"SD\",\n",
    "        \"kabbel\": \"jag har redan redogjort f\\u00f6r huvudargumentationen bakom detta,\"\n",
    "    }\n",
    "]\n",
    "\n",
    "test_samples,test_parti, test_classes = kabbel_to_vec(test_data)\n",
    "\n",
    "#test_samples = [\"Apan är Bäst\", \"Joakim är Bäst i klassen i alla fall\",\"Joakim\"]\n",
    "#test_classes = [\"Apan\", \"Joakim\",\"Joadkim\"]\n",
    "\n",
    "print(test_samples)\n",
    "a, b, c = samples_to_train(test_samples)\n",
    "print(a)\n",
    "samples_to_train(test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_data_info(speech,classes,test_index=0):\n",
    "    X_train,vocabulary_x, _ = samples_to_train(speech)\n",
    "    Y_train,vocabulary_y, _ = samples_to_train(classes)\n",
    "    print(\"Antal unika ord (features X):\\t\",X_train.shape[0])\n",
    "    print(\"training samples m:\\t\\t\",Y_train.shape[1])\n",
    "    print(\"unika klassificeringar (Y):\\t\",Y_train.shape[0])\n",
    "    vec, one = sample_to_vec(speech[test_index],vocabulary_x)\n",
    "    print(\"X:\")\n",
    "    print(\"Original: {0}{1}\".format(speech[test_index][:70],\"...\" if len(speech[test_index])>70 else \"\"))\n",
    "    print(\"Vokabulär: {0}\".format([x for x in vocabulary_x][:10]))\n",
    "    print(\"x0 :\\n{0}\".format(vec))\n",
    "    print(\"x0 compressed:\\n{0}\\n\".format(X_train)) \n",
    "    print(\"Y:\")\n",
    "    print([x for x in vocabulary_y])\n",
    "    print(\"y0 compressed:\\n{0}\".format(Y_train)) \n",
    "    assert X_train.shape[1] == Y_train.shape[1] , 'Det måste finnas lika många X som Y:n'\n",
    "\n",
    "def print_shape_info(X_train,Y_train):\n",
    "    print(\"number of training examples: {0}\".format(X_train.shape[1]))\n",
    "    print(\"X_train shape: {0}\".format(str(X_train.shape)))\n",
    "    print(\"Y_train shape: {0}\".format(str(Y_train.shape)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples: 5\n",
      "X_train shape: (9, 5)\n",
      "Y_train shape: (4, 5)\n",
      "Antal unika ord (features X):\t 9\n",
      "training samples m:\t\t 5\n",
      "unika klassificeringar (Y):\t 4\n",
      "X:\n",
      "Original: Apan är Bäst Bäst Bäst\n",
      "Vokabulär: ['är', 'fall', 'alla', 'i', 'Hejhej', 'klassen', 'Bäst', 'Apan', 'Joakim']\n",
      "x0 :\n",
      "[7 0 6 6 6]\n",
      "x0 compressed:\n",
      "[[ 1.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  2.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 3.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1.  0.  1.]]\n",
      "\n",
      "Y:\n",
      "['Apan', 'Joakim', 'Joadkim', 'Annan']\n",
      "y0 compressed:\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "test_samples = [\"Apan är Bäst Bäst Bäst\", \"Joakim är Bäst i klassen i alla fall\",\"Joakim\",\"Hejhej\",\"Joakim\"]\n",
    "test_classes = [\"Apan\", \"Joakim\",\"Joadkim\",\"Annan\",\"Joakim\"]\n",
    "\n",
    "\n",
    "\n",
    "X_samp,vocabulary_x, iw = samples_to_train(test_samples)\n",
    "Y_samp,vocabulary_y ,iw= samples_to_train(test_classes)\n",
    "\n",
    "print_shape_info(X_samp, Y_samp)\n",
    "print_data_info(test_samples,test_classes,test_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stop_words(X, iw, top=10):\n",
    "\n",
    "    X_samp_sum = np.sum(X,axis=1)\n",
    "    \n",
    "    idx = np.argsort(-X_samp_sum, axis=0)\n",
    "    iw_ar = [w for (i,w) in iw.items()]\n",
    "    count = X_samp_sum[idx[:top]].flatten()\n",
    "    labels = np.asarray(iw_ar)[idx[:top]]\n",
    "    a = count.tolist()[0]\n",
    "    b = labels.flatten().tolist()\n",
    "    z = dict(zip(b,a))\n",
    "    res = []\n",
    "    for k,v in z.items():\n",
    "        res.append(\"{0}\\t{1:20}\\t[{2}]\".format(len(res)+1,k,v).expandtabs(2))\n",
    "    print(\"\\n\".join(res))\n",
    "    return idx\n",
    "\n",
    "def remove_features(X_samp, del_idx, iw, remove):\n",
    "    #del_idx = get_stop_words(X_samp,iw,remove)\n",
    "    iwr = [w for (i,w) in iw.items()]\n",
    "    iwr = np.delete(iwr, del_idx[:remove], axis=0)\n",
    "    X_red = np.delete(X_samp, del_idx[:remove], axis=0)\n",
    "    \n",
    "    assert X_samp.shape[0] == X_red.shape[0]+remove\n",
    "    return X_red, iwr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Bäst                  [4.0]\n",
      "2 Joakim                [3.0]\n",
      "['är' 'fall' 'alla' 'i' 'Hejhej' 'klassen' 'Apan']\n"
     ]
    }
   ],
   "source": [
    "remove = 2\n",
    "\n",
    "X_samp,_, iw = samples_to_train(test_samples)\n",
    "del_idx = get_stop_words(X_samp,iw,remove)\n",
    "X_red, iwr = remove_features(X_samp,del_idx, iw, remove)\n",
    "\n",
    "print(iwr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(X,Y, seed=1):\n",
    "    np.random.seed(1)\n",
    "    randomize = np.arange(X.shape[1])\n",
    "    np.random.shuffle(randomize)\n",
    "    #blanda kolumnerna\n",
    "    X = X[:,randomize]\n",
    "    Y = Y[:,randomize]\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0.4359949 ,  0.02592623,  0.54966248,  0.43532239,  0.4203678 ],\n",
      "       [ 0.33033482,  0.20464863,  0.61927097,  0.29965467,  0.26682728]])\n",
      "array([[ 0.54966248,  0.02592623,  0.4203678 ,  0.4359949 ,  0.43532239],\n",
      "       [ 0.61927097,  0.20464863,  0.26682728,  0.33033482,  0.29965467]])\n",
      "array([[ 0.54966248,  0.02592623,  0.4203678 ,  0.4359949 ,  0.43532239],\n",
      "       [ 0.61927097,  0.20464863,  0.26682728,  0.33033482,  0.29965467]])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "a = np.random.rand(2,5)\n",
    "b,c  = shuffle(a,a,1)\n",
    "pp.pprint(a)\n",
    "pp.pprint(b)\n",
    "pp.pprint(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_dev_test(X,Y,ratio=.5):\n",
    "    np.random.seed(2)\n",
    "    m = X.shape[1]\n",
    "    m_train = int(np.floor((X.shape[1]*ratio)))\n",
    "    m_bi = int((m - m_train)/2)\n",
    "    \n",
    "    X_orig, Y_orig = shuffle(X,Y)\n",
    "    \n",
    "    X_train, X_dev, X_test = X_orig[:,:m_train], X_orig[:,m_train:m_train+m_bi], X_orig[:,m_train+m_bi:]\n",
    "    Y_train, Y_dev, Y_test = Y_orig[:,:m_train], Y_orig[:,m_train:m_train+m_bi], Y_orig[:,m_train+m_bi:]\n",
    "    print(m_train,m_bi+m_train)\n",
    "    return X_train, X_dev, X_test, Y_train, Y_dev, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "m:5\n",
      "m_train:4\n",
      "m_dev:0\n",
      "m_test:1\n",
      "[[ 1.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  2.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 3.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1.  0.  1.]]\n",
      "(9, 4)\n",
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print_data_info(speech,classes)\n",
    "x_train,vocabulary_x,_ = samples_to_train(test_samples)\n",
    "y_train,vocabulary_y,_ = samples_to_train(test_classes)\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = split_train_dev_test(x_train,y_train,ratio=.9)\n",
    "m = x_train.shape[1]\n",
    "print('m:{0}'.format(m))\n",
    "print('m_train:{0}'.format(X_train.shape[1]))\n",
    "print('m_dev:{0}'.format(X_dev.shape[1]))\n",
    "print('m_test:{0}'.format(X_test.shape[1]))\n",
    "\n",
    "print(x_train)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "assert m == Y_train.shape[1]+Y_dev.shape[1] + Y_test.shape[1]\n",
    "assert m == X_train.shape[1]+X_dev.shape[1] +X_test.shape[1] #\"matrisen harinte samma features\"\n",
    "assert X_train.shape[0] == X_dev.shape[0] #matrisen behåller formen features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x,n_y):\n",
    "    X = tf.placeholder(shape=[n_x,None],dtype=tf.float32,name=\"X\")\n",
    "    Y = tf.placeholder(shape=[n_y,None],dtype=tf.float32,name=\"Y\")\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiera parametrar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params(hyperparameters):\n",
    "    \"\"\"Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\"\"\"\n",
    "    \n",
    "    layer_dims = hyperparameters['layer_dims']\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = tf.get_variable(name=\"W\"+str(l),shape=[layer_dims[l],layer_dims[l-1]],initializer=tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "        parameters['b' + str(l)] = tf.get_variable(name=\"b\"+str(l),shape=[layer_dims[l],1],initializer=tf.zeros_initializer())\n",
    "\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_hyperparams(layer_dims):\n",
    "    hyperparameters = {}\n",
    "    hyperparameters['layer_dims'] = layer_dims\n",
    "    \n",
    "    return hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': <tf.Variable 'W1:0' shape=(25, 9) dtype=float32_ref>,\n",
      " 'W2': <tf.Variable 'W2:0' shape=(12, 25) dtype=float32_ref>,\n",
      " 'W3': <tf.Variable 'W3:0' shape=(4, 12) dtype=float32_ref>,\n",
      " 'b1': <tf.Variable 'b1:0' shape=(25, 1) dtype=float32_ref>,\n",
      " 'b2': <tf.Variable 'b2:0' shape=(12, 1) dtype=float32_ref>,\n",
      " 'b3': <tf.Variable 'b3:0' shape=(4, 1) dtype=float32_ref>}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.reset_default_graph() #nollställ modell\n",
    "hyperparameters = init_hyperparams(layer_dims=[X_train.shape[0],25,12,Y_train.shape[0]])\n",
    "paramteters = initialize_params(hyperparameters)\n",
    "\n",
    "pp.pprint(paramteters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, hyperparameters, parameters):\n",
    "    layer_dims = hyperparameters['layer_dims']\n",
    "    L = len(layer_dims)-1\n",
    "  \n",
    "    Zi = {}\n",
    "    Ai = {}\n",
    "    Ai[0] = X\n",
    "    for l in range(1,L+1):\n",
    "        Zi[l] = tf.matmul(parameters['W'+str(l)],Ai[l-1])+parameters['b'+str(l)]\n",
    "        Ai[l] = tf.nn.relu(Zi[l])\n",
    "        #print(\"ff\"+str(l)+\" \"+'W'+str(l)+\"x\"+\"Ai[\"+str(l-1)+\"] + \"+'b'+str(l))\n",
    "\n",
    "    return Zi[L]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z3 = Tensor(\"add_2:0\", shape=(4, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(X_train.shape[0], Y_train.shape[0])\n",
    "    parameters = initialize_params(hyperparameters)\n",
    "    Z3 = forward_propagation(X, hyperparameters, parameters)\n",
    "    print(\"Z3 = \" + str(Z3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z3 = Tensor(\"add_2:0\", shape=(4, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(X_train.shape[0], Y_train.shape[0])\n",
    "    parameters = initialize_params(hyperparameters)\n",
    "    Z3 = forward_propagation(X, hyperparameters, parameters)\n",
    "    print(\"Z3 = \" + str(Z3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kostnadsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \n",
    "    #    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    #biases = tf.Variable(tf.zeros([num_labels]))\n",
    "   # weights = tf.Variable(tf.truncated_normal([Z3.shape[0], Z3.shape[1]]))\n",
    "   # biases = tf.Variable(tf.zeros([Z3.shape[1]]))\n",
    "  \n",
    "    # Training computation.\n",
    "   # logits = tf.transpose(tf.matmul(Z3, weights) + biases )\n",
    "    logits = tf.transpose(Z3 + Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    # L2_regularization_cost = lambd/(2*m)*(np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3)))\n",
    "    #kostnadsfunktion ej regulariserad.\n",
    "    #logits = tf.matmul(tf_train_dataset, weights) + biases \n",
    "    # Original loss function\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) )\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels))\n",
    "    \n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(Z3, Y, parameters, lambd):\n",
    "    \n",
    "    layer_dims = hyperparameters['layer_dims']\n",
    "    L = len(layer_dims)-1\n",
    "    L2_regularization_cost = tf.Variable(0,dtype=tf.float32)\n",
    "    for l in range(1,L):\n",
    "        L2_regularization_cost +=  tf.nn.l2_loss(parameters['W'+str(l)])\n",
    "\n",
    "\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss + lambd * L2_regularization_cost)\n",
    "\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "layer_dims = hyperparameters['layer_dims']\n",
    "L = len(layer_dims)-1\n",
    "for l in range(1,L):\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X_train,vocabulary_x, iw = samples_to_train(test_samples)\n",
    "Y_train,vocabulary_y ,iw= samples_to_train(test_classes)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(X_train.shape[0], Y_train.shape[0])\n",
    "    parameters = initialize_params(hyperparameters)\n",
    "    Z3 = forward_propagation(X, hyperparameters, parameters)\n",
    "    cost = compute_cost_with_regularization(Z3, Y,parameters,.1)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X_train, Y_train, minibatch_size, seed):\n",
    "    \"\"\"Tar in hela train och delar upp i x antal minibatch_size\n",
    "        med randomiserade kolumner (träningsexempel)\"\"\"\n",
    "    cuts = int(X_train.shape[1] / minibatch_size)\n",
    "    X_train, Y_train = shuffle(X_train, Y_train, seed)\n",
    "    minibatch_X = []\n",
    "    minibatch_Y = []\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0,cuts+1):\n",
    "        minibatches.append((X_train[:,i*minibatch_size:(i+1)*minibatch_size],Y_train[:,i*minibatch_size:(i+1)*minibatch_size]))\n",
    "\n",
    "    return minibatches\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  7.  4.  7.  8.  6.  8.  4.  0.  3.  1.]\n",
      " [ 7.  6.  5.  7.  3.  3.  7.  4.  8.  1.  5.]\n",
      " [ 3.  2.  2.  7.  3.  2.  9.  4.  5.  3.  2.]]\n",
      "array([[ 4.,  7.,  8.],\n",
      "       [ 5.,  7.,  3.],\n",
      "       [ 2.,  7.,  3.]])\n",
      "array([[ 3.,  7.,  8.],\n",
      "       [ 1.,  6.,  7.],\n",
      "       [ 3.,  2.,  9.]])\n",
      "array([[ 1.,  4.,  1.],\n",
      "       [ 7.,  4.,  5.],\n",
      "       [ 3.,  4.,  2.]])\n",
      "array([[ 0.,  6.],\n",
      "       [ 8.,  3.],\n",
      "       [ 5.,  2.]])\n",
      "\n",
      "batch lengths: [3, 3, 3, 2]\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "a = np.floor(np.random.rand(3,11)*10)\n",
    "minibatches = random_mini_batches(a,a,3,1)\n",
    "\n",
    "print(a)\n",
    "for (_,b) in minibatches:\n",
    "    pp.pprint(b)\n",
    "\n",
    "print(\"\\nbatch lengths: {0}\".format([b.shape[1] for (_,b) in minibatches]))\n",
    "print(sum([b.shape[1] for (_,b) in minibatches]))\n",
    "assert sum([b.shape[1] for (_,b) in minibatches]) == a.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell\n",
    "Här börjar vi om fast använder ovan funktioner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 most used words:\n",
      "1 att                   [9167.0]\n",
      "2 det                   [7101.0]\n",
      "3 och                   [6519.0]\n",
      "4 i                     [5525.0]\n",
      "5 som                   [5008.0]\n",
      "6 är                    [4771.0]\n",
      "7 för                   [3785.0]\n",
      "8 har                   [3402.0]\n",
      "9 vi                    [3332.0]\n",
      "10  en                    [3128.0]\n",
      "11  på                    [2793.0]\n",
      "12  till                  [2449.0]\n",
      "13  om                    [2360.0]\n",
      "14  jag                   [2330.0]\n",
      "15  inte                  [2308.0]\n",
      "16  av                    [2209.0]\n",
      "17  de                    [2101.0]\n",
      "18  den                   [2092.0]\n",
      "19  med                   [2019.0]\n",
      "20  man                   [1780.0]\n",
      "21  ett                   [1738.0]\n",
      "22  ska                   [1692.0]\n",
      "23  kan                   [1161.0]\n",
      "24  men                   [987.0]\n",
      "25  detta                 [976.0]\n",
      "26  också                 [939.0]\n",
      "27  när                   [857.0]\n",
      "28  här                   [831.0]\n",
      "29  så                    [816.0]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "['offentligt' 'stor' 'eu-migranter' ..., 'tilläggsbudgeten' 'äkta'\n",
      " 'upplevelsen']\n",
      "13283 14943\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "{'C': 0, 'M': 1, 'TALMANNEN': 2, 'KD': 3, 'V': 4, 'S': 5, 'MP': 6, 'KONUNGEN': 7, 'L': 8, 'HANS': 9, 'SD': 10, 'MAJESTÄT': 11}\n",
      "train:(16506, 13283) dev:(16506, 1660) test:(16506, 1661)\n"
     ]
    }
   ],
   "source": [
    "jsondata = file_to_json(\"kabbel_big.txt\")\n",
    "speech, classes, person = kabbel_to_vec(jsondata)\n",
    "\n",
    "X_orig,vocabulary_x,iwx = samples_to_train(speech)\n",
    "Y_orig,vocabulary_y,iwy = samples_to_train(classes)\n",
    "\n",
    "#REMOVE STOP WORDS\n",
    "remove = 29\n",
    "print(\"{0} most used words:\".format(remove))\n",
    "del_idx = get_stop_words(X_orig,iwx,remove)\n",
    "X_orig, iwx_red = remove_features(X_orig,del_idx,iwx,remove)\n",
    "\n",
    "print(X_orig)\n",
    "row_sums = np.sum(X_orig,axis=1)\n",
    "print(iwx_red)\n",
    "#NORMALIZE X // not used\n",
    "#X_orig = X_orig / row_sums\n",
    "#print(X_orig)\n",
    "\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = split_train_dev_test(X_orig,Y_orig,ratio=.80)\n",
    "#print_data_info(speech,classes,test_index=0)\n",
    "\n",
    "print(vocabulary_y)\n",
    "print(\"train:{0} dev:{1} test:{2}\".format(X_train.shape,X_dev.shape,X_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "#epoch = antal iterationer över samma samples fast med olika rand. dist. (som montecarlo eller nått)\n",
    "\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.1,\n",
    "          num_epochs = 3000, minibatch_size = 512, print_cost = True, lambd = 0.03):\n",
    "\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results  \n",
    "    seed = 1                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "\n",
    "    #skapa tomma placeholders för vår data\n",
    "    X, Y = create_placeholders(n_x,n_y)\n",
    "    #skapa hyperparameter om lager och units samt aktiveringsfunktioner per lager\n",
    "    hyperparameters = init_hyperparams(layer_dims=[X_train.shape[0],10,5,Y_train.shape[0]])\n",
    "    #skapa W b i varje lager\n",
    "    parameters = initialize_params(hyperparameters)\n",
    "\n",
    "    #initiera beräkningsgrafen för forward/backward prop\n",
    "    Z3 = forward_propagation(X,hyperparameters,parameters)\n",
    "\n",
    "    #initiera beräkningsgraf för kostnaden \n",
    "    cost = compute_cost_with_regularization(Z3, Y, parameters, lambd) #compute_cost(Z3,Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    #nu med learning rate decay\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam').minimize(cost)\n",
    "   \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    epoch_print = 100\n",
    "    e_count = 0\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        model_start_time = time.time()\n",
    "        last_time = model_start_time\n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed +=1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "            \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "                \n",
    "              \n",
    "\n",
    "            # Print the cost every epoch\n",
    "            \n",
    "            if print_cost == True and (epoch % epoch_print == 0 and epoch > 0) or epoch == 1:\n",
    "                \n",
    "                t_now = time.time()\n",
    "                e_count = epoch - e_count\n",
    "                e_time = (t_now-last_time)/e_count\n",
    "               \n",
    "                est_end = (num_epochs - epoch)*e_time + t_now\n",
    "                st = datetime.datetime.fromtimestamp(est_end).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                if epoch == 1:\n",
    "                    print(\"{0}\".format(datetime.timedelta(seconds=(est_end-t_now))))\n",
    "                    #print(\"estimated: {:0>8}\".format(datetime.timedelta(seconds=66)))\n",
    "                print(\"Cost after epoch {0}: {1} - epoch time {2} - est end: {3}\".format(epoch, epoch_cost,e_time,st))\n",
    "                last_time = t_now\n",
    "            if print_cost == True and epoch % 10 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                if costs[0] < epoch_cost:\n",
    "                    print(\"SOMETHING IS WRONG - cost increase?!\")\n",
    "                    break\n",
    "                \n",
    "       \n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        print(hyperparameters)\n",
    "        \n",
    "         # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples: 13283\n",
      "X_train shape: (16506, 13283)\n",
      "Y_train shape: (12, 13283)\n",
      "0:21:14.524491\n",
      "Cost after epoch 1: 2.0660310983657837 - epoch time 6.404645681381226 - est end: 2017-09-21 16:37:56\n",
      "something is wrong- cost increase?!\n",
      "Parameters have been trained!\n",
      "Train Accuracy: 0.614921\n",
      "Test Accuracy: 0.28236\n",
      "{'layer_dims': [16506, 10, 5, 12]}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njDHGVLCiYIwxpoIVBWOMMRX+D3czlfXM+zrhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11dbce2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 143.2214493751526 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print_shape_info(X_train, Y_train)\n",
    "parameters = model(X_train, Y_train, X_test, Y_test, learning_rate = 0.01, num_epochs = 200,lambd = 0.0003)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cost after epoch 0: 1.944496\n",
    "Cost after epoch 100: 0.072316\n",
    "Cost after epoch 200: 0.060759\n",
    "Parameters have been trained!\n",
    "Train Accuracy: 0.992783\n",
    "Test Accuracy: 0.503597\n",
    "{'layer_dims': [16506, 25, 12, 12]}\n",
    "--- 43.270642042160034 seconds ---\n",
    "\n",
    "number of training examples: 2217\n",
    "X_train shape: (5165, 2217)\n",
    "Y_train shape: (5, 2217)\n",
    "4\n",
    "ff1 W1xAi[0] + b1\n",
    "ff2 W2xAi[1] + b2\n",
    "ff3 W3xAi[2] + b3\n",
    "ff4 W4xAi[3] + b4\n",
    "Parameters have been trained!\n",
    "Train Accuracy: 0.983762\n",
    "Test Accuracy: 0.528777\n",
    "--- 143.9702799320221 seconds ---\n",
    "\n",
    "\n",
    "number of training examples: 2217\n",
    "X_train shape: (5165, 2217)\n",
    "Y_train shape: (5, 2217)\n",
    "4\n",
    "ff1 W1xAi[0] + b1\n",
    "ff2 W2xAi[1] + b2\n",
    "ff3 W3xAi[2] + b3\n",
    "ff4 W4xAi[3] + b4\n",
    "Cost after epoch 0: 2.557675\n",
    "Cost after epoch 20: 2.010333\n",
    "Cost after epoch 40: 1.836701\n",
    "Cost after epoch 60: 1.696013\n",
    "Cost after epoch 80: 1.532787\n",
    "\n",
    "Parameters have been trained!\n",
    "Train Accuracy: 0.820478\n",
    "Test Accuracy: 0.57554\n",
    "--- 14.365275144577026 seconds ---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test av modellen på ny data\n",
    "en predict-funktion som kör modellens forward-pass och använder softmax för klassificering till någon av de giltiga Y-värdena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(speech, parameters, vocabulary):\n",
    "        sample_to_vec(speech, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X_samp):\n",
    "    X = tf.placeholder(shape=[X_train.shape[0],None],dtype=tf.float32,name=\"X\")\n",
    "    pred = forward_propagation(X,hyperparameters,parameters)\n",
    "    classify = tf.nn.softmax(tf.transpose(pred))\n",
    "    session = tf.Session()\n",
    "    session.run(init)\n",
    "    b = session.run(classify, feed_dict={X: X_samp})\n",
    "    return(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"X_1:0\", shape=(16506, ?), dtype=float32) must be from the same graph as Tensor(\"W1:0\", shape=(25, 9), dtype=float32_ref).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-ef50e077b2ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#XS_train, XS_dev, XS_test, YS_train, YS_dev, YS_test = split_train_dev_test(speech,classes,ratio=.82)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msample_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-abfcd2529478>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(X_samp)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_samp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mclassify\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-087ba1c721bf>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(X, hyperparameters, parameters)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mAi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mZi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mAi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#print(\"ff\"+str(l)+\" \"+'W'+str(l)+\"x\"+\"Ai[\"+str(l-1)+\"] + \"+'b'+str(l))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/y-/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1747\u001b[0m       \u001b[0mare\u001b[0m \u001b[0mboth\u001b[0m \u001b[0mset\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m   \"\"\"\n\u001b[0;32m-> 1749\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0madjoint_a\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Only one of transpose_a and adjoint_a can be True.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/y-/anaconda/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/y-/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname_scope\u001b[0;34m(name, default_name, values)\u001b[0m\n\u001b[1;32m   4165\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4166\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4167\u001b[0;31m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4168\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4169\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/y-/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   3910\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3911\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3912\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3913\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3914\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/Users/y-/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   3849\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m     raise ValueError(\n\u001b[0;32m-> 3851\u001b[0;31m         \"%s must be from the same graph as %s.\" % (item, original_item))\n\u001b[0m\u001b[1;32m   3852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"X_1:0\", shape=(16506, ?), dtype=float32) must be from the same graph as Tensor(\"W1:0\", shape=(25, 9), dtype=float32_ref)."
     ]
    }
   ],
   "source": [
    "#XS_train, XS_dev, XS_test, YS_train, YS_dev, YS_test = split_train_dev_test(speech,classes,ratio=.82)\n",
    "sample_i = 1000\n",
    "b = predict(X_test[:,sample_i])\n",
    "c = np.zeros(b.shape[1]).astype(int)\n",
    "\n",
    "c[np.argmax(b)] = 1\n",
    "print(speech[sample_i])\n",
    "print(iwy[np.argmax(b)])\n",
    "print(classes[(sample_i-3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def samples_to(speech,classes,idx):\n",
    "    xvocabulary, iwx = samples_to_vocab(speech)\n",
    "    yvocabulary, iwy = samples_to_vocab(classes)\n",
    "    \n",
    "    _, x = sample_to_vec(speech[idx], xvocabulary)\n",
    "    _, y = sample_to_vec(classes[idx], yvocabulary)\n",
    "    return speech[idx],classes[idx],x.T,y, iwx,iwy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_vec(sentence,vocabular):\n",
    "    sample_words = np.array(sentence.split(\" \"))\n",
    "    vec = np.array([vocabular.get(x) for x in sample_words if vocabular.get(x) is not None])\n",
    "    one_hot = np.zeros([len(vec), len(vocabular)])\n",
    "    one_hot[np.arange(len(vec)),vec] = 1\n",
    "    one_hot = np.sum(one_hot,axis=0)\n",
    "    return one_hot, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_predict(sentence,full_vocabulary,classY):\n",
    "    str_s = sentence.lower()\n",
    "    jvoc = np.asarray([k for k,v in full_vocabulary.items()])\n",
    "    print(jvoc.shape)\n",
    "    jvoc, iwx_red = remove_features(jvoc,del_idx,iwx,remove)\n",
    "\n",
    "    jvoc = dict(zip(jvoc,range(len(jvoc))))\n",
    "    \n",
    "    xr,_ =sentence_to_vec(str_s,jvoc)\n",
    "    xr = xr.reshape(xr.shape[0],1)\n",
    "    pred = predict(xr)\n",
    "    c = np.zeros(pred.shape[1]).astype(int)\n",
    "    c[np.argmax(pred)] = 1\n",
    "    \n",
    "    print(\"{0}\\n{1} ---> {2}\\n{3}\\n\".format(str_s,x,pred,classY[np.argmax(pred)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16506, 16604)\n",
      "(16535,)\n",
      "(16535,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"X_2:0\", shape=(16506, ?), dtype=float32) must be from the same graph as Tensor(\"W1:0\", shape=(25, 9), dtype=float32_ref).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-f734574c8b3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mjvoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvoc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtest_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Man tycker ju som lekman att det i lagtexterna också skulle kunna fogas in någon formulering att om ett möte hindrar andra att hålla sina möten\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miwy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-226ad10d18d3>\u001b[0m in \u001b[0;36mtest_predict\u001b[0;34m(sentence, full_vocabulary, classY)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mxr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0msentence_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mxr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-abfcd2529478>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(X_samp)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_samp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mclassify\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-087ba1c721bf>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(X, hyperparameters, parameters)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mAi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mZi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mAi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#print(\"ff\"+str(l)+\" \"+'W'+str(l)+\"x\"+\"Ai[\"+str(l-1)+\"] + \"+'b'+str(l))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/y-/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1747\u001b[0m       \u001b[0mare\u001b[0m \u001b[0mboth\u001b[0m \u001b[0mset\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m   \"\"\"\n\u001b[0;32m-> 1749\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0madjoint_a\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Only one of transpose_a and adjoint_a can be True.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/y-/anaconda/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/y-/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname_scope\u001b[0;34m(name, default_name, values)\u001b[0m\n\u001b[1;32m   4165\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4166\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4167\u001b[0;31m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4168\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4169\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/y-/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   3910\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3911\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3912\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3913\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3914\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/Users/y-/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   3849\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m     raise ValueError(\n\u001b[0;32m-> 3851\u001b[0;31m         \"%s must be from the same graph as %s.\" % (item, original_item))\n\u001b[0m\u001b[1;32m   3852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"X_2:0\", shape=(16506, ?), dtype=float32) must be from the same graph as Tensor(\"W1:0\", shape=(25, 9), dtype=float32_ref)."
     ]
    }
   ],
   "source": [
    "print(X_orig.shape)\n",
    "str_s,str_c,x,y,iwx,iwy =    samples_to(speech,classes,1)\n",
    "sample_i = 2622\n",
    "\n",
    "jvoc = np.asarray([k for k,v in vocabulary_x.items()])\n",
    "print(jvoc.shape)\n",
    "jvoc, iwx_red = remove_features(jvoc,del_idx,iwx,remove)\n",
    "\n",
    "jvoc = dict(zip(jvoc,range(len(jvoc))))\n",
    "\n",
    "test_predict(\"Man tycker ju som lekman att det i lagtexterna också skulle kunna fogas in någon formulering att om ett möte hindrar andra att hålla sina möten\",vocabulary_x,iwy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
